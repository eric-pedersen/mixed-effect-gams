
# V: Computational and statistical issues when fitting HGAMs

Which of the five model formulations should you choose for a given data set? There are two major trade-offs to consider. The first is the bias-variance trade-off: more complex models can account for more fluctuations in the data, but also tend to give more variable predictions, and can overfit.  The second trade-off is model complexity versus computational cost: more complex models can include more potential sources of variation and give more information about a given data set, but will generally take more time and computational resources to fit and debug. We discuss both of these trade-offs in this section. We also discuss
how to extend the HGAM framework to fit more complex models.

## Bias-variance trade-offs

The bias-variance trade-off is a fundamental concept in statistics. When trying to estimate any relationship (in the case of GAMs, a smooth relationship between predictors and data) bias measures how far, on average, an estimate is from the true value.
The variance of an estimator corresponds to how much that estimator would fluctuate if applied to multiple different samples of the same size taken from the
same population.
These two properties tend to be traded off when fitting models.
For instance, rather than estimating a population mean from data, we could simply use a predetermined fixed value regardless of the observed data[^mean_note].
This estimate would have no variance (as it is always the same regardless of what the data look like) but would have high bias unless the true population mean happened to equal zero. 
Penalization is useful because using a penalty term slightly increases model bias, but can substantially decrease variance [@efron_steins_1977].

In GAMs, the bias-variance trade-off is managed by the terms of the penalty matrix, and equivalently random effect variances in HGLMs.
Larger penalties correspond to lower variance, as the estimated function is unable to wiggle a great deal, but also correspond to higher bias unless the true function is close to the null space for a given smoother (e.g., a straight line for thin plate splines with 2nd derivative penalties, or zero for a random effect).
The computational machinery used by **mgcv** to fit smooth terms is designed to find penalty terms that best trade-off bias for variance to find a smooth that can effectively predict new data.

The bias-variance trade-off comes into play with HGAMs when choosing whether to
fit separate penalties for each group level or assign a common penalty for all
group levels (i.e., deciding between models 2 & 3 or models 4 & 5). If the
functional relationships we are trying to estimate for different group levels
actually vary in how wiggly they are, setting the penalty for all group-level
smooths equal (models 2 & 4) will either lead to overly variable estimates for the
least variable group levels, over-smoothed (biased) estimates for the most
wiggly terms, or a mixture of these two, depending on the fitting criteria.

```{r single_smooth_bias, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(1)

calc_2nd_deriv = function(x,y){
  deriv_val = (lag(y) + lead(y) - 2*y)/(x-lag(x))^2
  deriv_val
}

freq_vals = c(1/4,1/2,1,2,4)
dat = crossing(x = seq(0,2*pi,length=150),freq = freq_vals)%>%
  mutate(y = sin(freq*x) +rnorm(n(), 0, 0.2),
         grp = paste("frequency = ",freq,sep= ""),
         grp = factor(grp,  levels = paste("frequency = ",freq_vals,sep= "")))

mod1 = bam(y~s(x,k=30,grp, bs="fs"), data=dat)
mod2 = bam(y~s(x,k=30,by=grp)+s(grp,bs="re"), data=dat)

overfit_predict_data = crossing(x = seq(0,2*pi,length=500), freq = freq_vals)%>%
  mutate(grp = paste("frequency = ",freq,sep= ""),
         grp = factor(grp,  levels = paste("frequency = ",freq_vals,sep= "")),
         y = sin(freq*x))%>%
  mutate(fit1 = as.numeric(predict(mod1,newdata = .,type = "response")),
         fit2 = as.numeric(predict(mod2,newdata = .,type="response")))

overfit_predict_data_long = overfit_predict_data %>%
  gather(model, value, y, fit1, fit2)%>%
  mutate(model = recode(model, y = "true value",fit1 = "model 4 fit",
                        fit2 = "model 5 fit"),
         model = factor(model, levels=  c("true value","model 4 fit",
                                          "model 5 fit")))

deriv_est_data = overfit_predict_data%>%
  group_by(grp)%>%
  arrange(grp, x)%>%
  mutate(fit1_deriv = calc_2nd_deriv(x,fit1),
         fit2_deriv = calc_2nd_deriv(x,fit2))%>%
  summarize(freq = freq[1], fit1_int = sum(fit1_deriv^2*(x-lag(x)),na.rm = T),
            fit2_int = sum(fit2_deriv^2*(x-lag(x)),na.rm = T))%>%
  ungroup()%>%
  mutate(sqr_2nd_deriv = -freq^3*(sin(4*pi*freq)-4*pi*freq)/4)%>%
  gather(key=model,value = obs_sqr_deriv,fit1_int,fit2_int)%>%
  mutate(model = factor(ifelse(model=="fit1_int", "model 4 fit",
                               "model 5 fit"),
                        levels = c("model 4 fit",
                                   "model 5 fit")))

deriv_plot =  ggplot(data=deriv_est_data, aes(x=sqr_2nd_deriv, y= obs_sqr_deriv,color= model))+
  geom_point()+
  scale_y_log10("integral of squared second\nderivative for fitted curves")+
  scale_x_log10("integral of squared second\nderivative for true curve")+
  scale_color_brewer(name=NULL,palette= "Set1")+
  geom_abline(color="black")+
  theme_bw()+
  theme(legend.position = "top")

fit_colors = c("black",RColorBrewer::brewer.pal(3, "Set1")[1:2])

overfit_vis_plot = ggplot(data=overfit_predict_data_long,aes(x=x,y= value,color=model))+
  geom_line()+
  scale_color_manual(values=fit_colors)+
  facet_grid(.~grp)+
  theme_bw()+
  theme(legend.position = "top")
```

We developed a simple numerical experiment to determine whether **mgcv**'s fitting
criteria tend to set estimated smoothness penalties high or low in the presence
of among-group variability in smoothness when fitting model 2 or 4 HGAMs. We
simulated data from five different groups, with all groups having the same
levels of the covariate $x$, ranging from 0 to $2\pi$.
For each group, the true function relating $x$ to the response, $y$, was a sine wave, but the frequency varied from 0.25 (equal to half a cycle across the range of $x$) to 4 (corresponding to 4 full cycles across the range).
We added normally distributed error to all $y$-values, with a standard deviation of 0.2.
We then fit both model 4 (where all curves were assumed to be equally smooth) and model 5 (with varying smoothness) to the entire data set, using REML criteria to estimate penalties.
For this example (Fig. \ref{fig:var_pen}a), requiring equal smoothness for all group levels resulted in **mgcv** underestimating the penalty for the lowest frequency (most smooth) terms, but accurately estimating the true smoothness of the highest frequency terms as measured by the squared second derivative of the smooth fit versus that of the true function (Fig. \ref{fig:var_pen}b).
This implies that assuming equal smoothness will result in underestimating
the true smoothness of low-variability terms, and thus lead to more variable
estimates of these terms.
If this is a potential issue, we recommend fitting both models 4 and 5 and using standard model evaluation criteria (e.g., AIC) to determine if there is evidence for among-group variability in smoothness.
For instance, the AIC for model 4 fit to this data is `r round(AIC(mod1))`, whereas it is `r round(AIC(mod2))` for model 5, implying a substantial improvement in fit by allowing smoothness to vary.
However, it may be the case that there are too few data points per group to estimate separate smoothness levels, in which case model 2 or model 4 may still be the better option even in the face of varying smoothness.

The ideal case would be to assume that among-group penalties follow their own distribution (estimated from the data), to allow variation in smoothness while still getting the benefit of pooling information on smoothness between groups.
This is currently not implemented in  **mgcv**.
It is possible to set up this type of varying penalty model in flexible Bayesian modelling software such as *Stan* (see below for a discussion of how to fit HGAMs using these tools), but how to this type of model has not been well studied.

[^mean_note]: While this example may seem contrived, this is exactly what happens
when we assume a given regression coefficient is equal to zero (and thus exclude it from a model).

```{r single_smooth_bias_plot, echo=FALSE, fig.width=6, fig.height=6, message=FALSE, warning=FALSE, cache=TRUE, fig.cap="\\label{fig:var_pen} a) Illustration of bias that can arise from assuming equal smoothness for all group levels (model 4, red line) versus allowing for intergroup variation in smoothness (model 5, red line) when the true function (black line) shows substantial variation in smoothness between groups. b) Estimated wiggliness (as measured by the integral of the squared 2nd derivative) of the true function for each group level versus that for the functions estimated by model 4 (red) and model 5 (blue), indicating substantial undersmoothing for low-variability curves by model 4."}
cowplot::plot_grid(overfit_vis_plot, deriv_plot, ncol=1, labels="auto",
                   align="hv", axis="lrtb")
```

It may seem there is also a bias-variance trade-off between choosing to use a single global smoother (model 1) or a global smoother plus group-level terms (models 2 and 3).
In model 1, all the data is used to estimate a single smooth term, and thus should have lower variance than models 2 and 3, but higher bias for any given group in the presence of inter-group functional variability.
However, in practice, this trade-off will be handled via penalization; if there are no average differences between functional responses, **mgcv** will penalize the group-specific functions toward zero, and thus toward the global model.
The choice between using model 1 versus models 2 and 3 should generally be driven by computational costs.
Model 1 is typically much faster to fit than models 2 and 3, even in the absence of among-group differences.
If there is no need to estimate inter-group variability, model 1 will typically be
more efficient.

A similar issue exists when choosing between models 2 and 3 and models 4 and 5.
If all group levels have very different functional shapes, the global term will get penalized toward zero in models 2 and 3, so they will reduce to models 4 and 5.
The choice to include a global term should be made based on scientific considerations (is the global term of interest?) and computational considerations.

## Complexity-computation trade-offs

The more flexible a model is, the larger an effective parameter space any fitting software has to search. It can be surprisingly easy to use massive computational resources trying to fit models to even small datasets. While we typically want to select models based on their fit and our inferential goals, computing resources can often act as an effective upper bound on model complexity. For a given data set, assuming a fixed family and link function, the time taken to estimate an HGAM will depend (roughly) on four factors: *(i)* the number of basis functions to be estimated, *(ii)* the number of smoothing parameters to be estimated, *(iii)* whether the model needs to estimate both a global smooth and groupwise smooths, and *(iv)* the algorithm and fitting criteria used to estimate parameters.

The most straightforward factor that will affect the amount of computational resources is the number of parameters in the model. Adding group-level smooths (moving from model 1 to 2-5) means that there will be more regression parameters to estimate. For a dataset with $n_g$ different groups and $n$ data, fitting a model with just a global smooth, `y~s(x,k=k)` will require $k$ coefficients, and takes $\mathcal{O}(nk^2)$ operations to evaluate.  Fitting the same data using a group-level smooth (model 4, `y~s(x,fac,bs="fs",k=k)`) will require $\mathcal{O}(nk^2g^2)$ operations to evaluate.  In effect, adding a group-level smooth will increase computational cost by an order of the number of groups squared. The effect of this is visible in the examples we fit in section III.  Table \ref{tab:comp_time} compares the relative time it takes to compute model 1 versus the other models.

One way to deal with this issue would be to reduce the number of basis functions used when fitting group-level smooths when the number of groups is large, limiting the flexibility of the model. It can also make sense to use more computationally-efficient basis functions when fitting large data sets, such as P-splines [@wood_p_splines_2017] or cubic splines.  Thin plate splines entail greater computational costs [@wood_generalized_2017].

Including a global smooth (models 2 and 3 compared to models 4 and 5) will not generally substantially affect the number of coefficients that need to be estimate (Table \ref{tab:comp_time}).
Adding a global term will add at most `k` extra terms.
It can be substantially less than that, as **mgcv** drops basis functions from co-linear smooths to ensure that the model matrix is full rank.

Adding additional smoothing parameters (moving from model 2 to 3, or moving from model 4 to 5) is more costly than increasing the number of coefficients to estimate, as estimating smoothing parameters is computationally intensive [@wood_fast_2011].
This means that models 2 and 4 will generally be substantially faster than 3 and 5 when the number of groups is large, as models 3 and 5 fit a separate set of penalties for each group level. The effect of this is visible in comparing the time it takes to fit model 2 to model 3 (which has a smooth for each group) or models 4 and 5 for the example data (Table \ref{tab:comp_time}). Note that this will not hold in all cases. For instance, model 5 takes less time to fit the bird movement data than model 4 does (Table \ref{tab:comp_time}B).


```{r comp_calc, echo=FALSE,  fig.width=4, fig.height=6, message=FALSE, warning=FALSE, cache=TRUE}
#Note: this code takes quite a long time to run! It's fitting all 10 models.
#Run once if possible, then rely on the cached code. There's a reason it's split off from the rest of the chunks of code.
source("../code/functions.R")

get_n_pen  = function(model) {
  family = model$family[[1]]
  if(family %in% c("Gamma","gaussian")){
    capture.output({out_val = nrow(gam.vcomp(model))-1})
  }else{
    capture.output({out_val = nrow(gam.vcomp(model))})
  }
  return(out_val)
}

get_n_coef = function(model) length(coef(model))

get_n_iter = function(model) model$outer.info$iter
get_n_out_iter = function(model) model$iter

bird_move = read.csv("../data/bird_move.csv")

comp_resources = crossing(model_number = c("1","2","3","4","5"),
                       data_source = factor(c("CO2","bird_move"),
                                     levels = c("CO2","bird_move")),
                       time = 0, n_smooths = 0,
                       n_coef = 0)

CO2$Plant_uo = factor(CO2$Plant, levels = levels(CO2$Plant), ordered = F)


comp_resources[1,"time"] = system.time(CO2_mod1 <- gam(log(uptake) ~ s(log(conc),k=5,m=2, bs="tp")+s(Plant_uo, k =12,  bs="re"),
                                                    data= CO2,method="REML",
                                                    control = list(keepData=TRUE)))[3]

comp_resources[2,"time"] = system.time(bird_mod1 <- gam(count ~ te(week,latitude, bs= c("cc", "tp"), k=c(10,10)),
                                                     data= bird_move, method="REML", family= poisson,
                                                     control = list(keepData=TRUE)))[3]

comp_resources[3,"time"] = system.time(CO2_mod2 <- gam(log(uptake) ~ s(log(conc),k=5,m=2, bs="tp")+
                                                       s(log(conc), Plant_uo, k=5, bs="fs",m=1),
                                                     data= CO2,method="REML",
                                                     control = list(keepData=TRUE)))[3]


comp_resources[4,"time"] = system.time(bird_mod2 <- gam(count ~ te(week,latitude, bs= c("cc", "tp"),
                                                                 k=c(10,10),m=c(2,2))+
                                                        te(week,latitude,species, bs= c("cc", "tp","re"),
                                                           k=c(10,10,6),m = c(1,1,1)),
                                                      data= bird_move, method="REML", family= poisson,
                                                      control = list(keepData=TRUE)))[3]


comp_resources[5,"time"] = system.time(
  CO2_mod3 <- gam(log(uptake) ~ s(log(conc),k=5,m=2, bs="tp")+
                    s(log(conc),by= Plant_uo, k =5,  bs="ts",m=1)+
                    s(Plant_uo,bs="re",k=12),
                  data= CO2,method="REML",
                  control = list(keepData=TRUE)))[3]



comp_resources[6,"time"] = system.time(
  bird_mod3 <- gam(count ~ te(week,latitude, bs= c("cc", "tp"),
                              k=c(10,10),m=c(2,2)) +
                     te(week,latitude, bs= c("cc", "tp"),
                        k=c(10,10),m=c(1,1),by= species),
                   data= bird_move, method="REML", family= poisson,
                   control = list(keepData=TRUE)))[3]


comp_resources[7,"time"] = system.time(
  CO2_mod4 <- gam(log(uptake) ~ s(log(conc), Plant_uo, k=5,  bs="fs",m=2),
                  data= CO2,method="REML",
                  control = list(keepData=TRUE)))[3]


comp_resources[8,"time"] = system.time(
  bird_mod4 <- gam(count ~ te(week,latitude,species, bs= c("cc", "tp","re"),
                              k=c(10,10,6),m = 2),
                   data= bird_move, method="REML", family= poisson,
                   control = list(keepData=TRUE))
)[3]


comp_resources[9,"time"] = system.time(
  CO2_mod5 <- gam(log(uptake) ~ s(log(conc),by= Plant_uo, k =5,  bs="tp",m=2)+
                    s(Plant_uo,bs="re",k=12), data= CO2,method="REML",
                  control = list(keepData=TRUE))

)[3]

comp_resources[10,"time"] = system.time(
  bird_mod5 <- gam(count ~ te(week,latitude,by=species, bs= c("cc", "tp"),
                              k=c(10,10),m = 2),
                   data= bird_move, method="REML", family= poisson,
                   control = list(keepData=TRUE))
)[3]


comp_resources$model = list(CO2_mod1, bird_mod1, CO2_mod2, bird_mod2,
                            CO2_mod3, bird_mod3,CO2_mod4, bird_mod4,
                            CO2_mod5, bird_mod5)

comp_resources = comp_resources %>%
  group_by(model_number, data_source)%>%
  mutate(n_smooths = get_n_pen(model[[1]]),
         n_coef = get_n_coef(model[[1]]),
         n_iter = get_n_iter(model[[1]]),
         n_iter_out = get_n_out_iter(model[[1]]))

```


```{r comp_time, echo=FALSE, fig.width=4, fig.height=6, message=FALSE, warning=FALSE, cache=TRUE}

comp_resources_table =comp_resources %>%
  ungroup()%>%
  arrange(data_source,model_number)%>%
  transmute(data_source =data_source, model=model_number,
            `relative time` = time,`coefficients` = n_coef,
            `penalties` = n_smooths
            )%>%
  group_by(data_source) %>%
  mutate(`relative time` = `relative time`/`relative time`[1],#scales processing time relative to model 1
         `relative time` = ifelse(`relative time`<10, signif(`relative time`,1), signif(`relative time`, 2)) #rounds to illustrate differences in timing.
         )%>%
  ungroup()%>%
  select(-data_source)

kable(comp_resources_table,format ="latex", caption="Relative computational time and model complexity for different HGAM formulations of the two example data sets from section III. All times are scaled relative to the length of time model 1 takes to fit to that data set. The number of coefficients measures the total number of model parameters (including intercepts). The number of smooths is the total number of unique penalty values estimated by the model.", booktabs = T)%>% #NOTE: change format to "latex" when compiling to pdf, "html" when compiling html
  kable_styling(full_width = F)%>%
  add_header_above(c(" " = 1," "=1, "# of terms"=2))%>%
  group_rows("A. CO2 data", 1,5)%>%
  group_rows("B. bird movement data", 6,10)

```

## Alternative formulations: `bam()`, `gamm()`, and `gamm4()`

When fitting models with large numbers of groups, it is often possible to speed up computation substantially by using one of the alternative fitting routines available through **mgcv**.

The first option is the funcion `bam()`, this requires the least changes to existing code written using the `gam()` function. `bam()` is designed to improve  performance when fitting large data sets via two mechanisms. First, it saves on memory needed to compute a given model by using a random subset of the data to calculate the basis functions. It then blocks the data and updates model fit within each block [@wood_generalized_2015]. While this is primarily designed to reduce memory usage, it can also substantially reduce computation time. Second, when using `bam()`'s default fREML ("Fast REML") method, you can use the `discrete=TRUE` option: this discretizes each covariate, substantially reducing the amount of computation needed (@Wood2017-iy; see `?mgcv::bam` for more details).

`bam()` has a larger computational overhead than `gam()`, so for small numbers of groups, it can be slower than `gam()` (Figure\ \ref{fig:alt_timing}). As the number of groups increases, computational time for `bam()` increases more slowly than for `gam()`; in our simulation tests, when the number of groups is greater than 16, `bam()` can be upward of an order of magnitude faster (Figure \ref{fig:alt_timing}). Note that `bam()` can be somewhat less computationally stable when estimating these models (i.e., less likely to converge).

The second option is to fit models using one of two dedicated mixed effect
model estimation packages, **nlme** and **lme4**. The **mgcv** package includes the
function `gamm()`, which uses the **nlme** package to estimate the GAM,
automatically handling the transformation of smooth terms into random effects
(and back into basis function representations for plotting and other statistical
analyses). The `gamm4()` function, in the separate **gamm4** package, uses **lme4** in a similar way. Using `gamm()` or `gamm4()` to fit models
rather than `gam()` can substantially speed up computation when the number of
groups is large, as both **nlme** and **lme4** take advantage of the sparse
structure of the random effects, where most basis functions will be zero for
most groups (i.e., any group-specific basis function will only take a non-zero
value for observations in that group level). As with `bam()`, `gamm()` and `gamm4()` are generally slower than `gam()` for fitting HGAMs when the number of group
levels is small (in our simulations, <8 group levels), however they do show
substantial speed improvements even with a moderate number of groups, and were
as fast as or faster to calculate than `bam()` for all numbers of grouping levels
we tested (Figure \ref{fig:alt_timing})[^parallel].

<!-- send to discussion For large numbers of group
levels and bigger data sets, it may be necessary to use full functional
regression methods such as those implemented in the `refund` package
[@scheipl_functional_2014]. See below for a discussion of what functional
regression is and its connections to HGAMs. -->

[^parallel]: It is also possible to speed up both `gam()` and `bam()` by using
multiple processors in parallel, whereas this is not currently possible for
`gamm()` and `gamm4()`. For large numbers of grouping levels, this should speed up
computation as well, at the cost of using more memory. However, computation time
will likely not decline linearly with the number of cores used, since not all
model fitting sets are parallelizable, and performance of cores can vary. As
parallel processing can be complicated and dependent on the type of computer
you are using to configure, we do not go into how to use these methods
here. The help file `?mgcv::mgcv.parallel` explains how to use parallel
computations for `gam()` and `bam()` in detail.

```{r alt_model_timing_plot, echo=FALSE, fig.width=6, fig.height=4, message=FALSE, warning=FALSE, cache=TRUE, purl=TRUE, fig.cap = "\\label{fig:alt_timing}Elapsed time to estimate the same model using each of the four approaches. Each data set was generated with 20 observations per group using a unimodal global function and random group-specific functions consisting of an intercept, a quadratic term, and logistic trend for each group. Observation error was normally distributed. Models were fit using model 2: \\texttt{y~s(x, k=10, bs=\"cp\") + s(x,fac, k=10, bs=\"fs\", xt=list(bs=\"cp\"), m=1)}. All models were run on a single core."}
set.seed = 1 # ensures that each new model parameter set is an extension of the old one

n_x = 20
x = seq(-2,2, length=n_x)
n_steps = 7

fit_timing_data = data_frame(n_groups = 2^(1:n_steps),
                             gam=rep(0,length=n_steps),
                             `bam (discrete = FALSE)`= 0,
                             `bam (discrete = TRUE)` = 0,
                             gamm = 0, gamm4 = 0)

fac_all = paste("g", 1:max(fit_timing_data$n_groups),sep = "")

model_coefs_all = data_frame(fac=fac_all)%>%
  mutate(int = rnorm(n(), 0,0.1),
         x2  = rnorm(n(),0,0.2),
         logit_slope = rnorm(n(),0, 0.2))

for(i in 1:n_steps){

  n_g =  fit_timing_data$n_groups[i]
  
  fac_current = fac_all[1:n_g]
  fac_current = factor(fac_current, levels=  unique(fac_current))

  model_coefs = model_coefs_all%>%
    filter(fac %in% fac_current)%>%
    mutate(fac = factor(fac, levels= unique(fac)))
  
  set.seed = 1 #ensures that each new data set is an extension of the old one
  
  model_data = crossing(fac=fac_current, x=x)%>%
    left_join(model_coefs)%>%
    mutate(base_func  = dnorm(x)*10,
           indiv_func = int + x^2*x2 + 2*(exp(x*logit_slope)/(1+exp(x*logit_slope))-0.5),
           y = base_func + indiv_func + rnorm(n()))
  
  fit_timing_data$gam[i] = system.time(gam(y~s(x,k=10, bs="cp") + s(x,fac, k=10, bs="fs", xt=list(bs="cp"), m=2),
                             data= model_data, method="REML"))[3]
  
  fit_timing_data$`bam (discrete = FALSE)`[i] = system.time(bam(y~s(x,k=10, bs="cp") + s(x,fac, k=10, bs="fs", xt=list(bs="cp"),m=2),
                             data= model_data, discrete = FALSE))[3]
  
  fit_timing_data$`bam (discrete = TRUE)`[i] = system.time(bam(y~s(x,k=10, bs="cp") + s(x,fac, k=10, bs="fs", xt=list(bs="cp"),m=2),
                                                                data= model_data,discrete=TRUE))[3]
  
  
  fit_timing_data$gamm[i] = system.time(gamm(y~s(x,k=10, bs="cp") + s(x,fac, k=10, bs="fs", xt=list(bs="cp"),m=2),
                               data= model_data))[3]
  
  
  fit_timing_data$gamm4[i] = system.time(gamm4(y~s(x,k=10, bs="cp") + s(x,fac, k=10, bs="fs", xt=list(bs="cp"),m=2),
                                 data= model_data))[3]
}


fit_timing_long = fit_timing_data %>% 
  gather(model, timing, gam,`bam (discrete = FALSE)`, 
         `bam (discrete = TRUE)`, gamm, gamm4)%>%
  mutate(model =factor(model, levels = c("gam",
                                         "bam (discrete = FALSE)",
                                         "bam (discrete = TRUE)",
                                         "gamm", 
                                         "gamm4")))


timing_plot = ggplot(aes(n_groups, timing, color=model, linetype= model), 
                     data=fit_timing_long)+
  geom_line()+
  geom_point()+
  scale_color_manual(values = c("black", "#1b9e77","#1b9e77", "#d95f02", "#7570b3"))+
  scale_linetype_manual(values =c(1,1,2,1,1))+
  scale_y_log10("run time (seconds)", breaks = c(0.1,1,10,100), labels = c("0.1", "1","10", "100"))+
  scale_x_log10("number of groups", breaks = c(2,8,32,128))+
  
  theme_bw()+
  guides(color = guide_legend(nrow = 2, byrow = TRUE))+
  theme(legend.position = "top")
timing_plot
```

Setting up models 1-5 in `bam()` uses the same code as we have previously covered;
the only difference is that you use the `bam()` instead of `gam()` function, and
have the additional option of discretizing your covariates. The advantage of
this approach is that `bam()` allows you to use almost all of the same families
available to the `gam()` function, and `bam()` model output can be evaluated using
the same functions (e.g., `summary`, `AIC`, `plot`, etc.) so it is simple to
substitute for `gam()` if you need to speed a model up.

Both `gamm()` and `gamm4()` require a few changes to model code.
First, there are a few limitations on how you are able to specify models 1-5 in both frameworks. Factor-smooth interaction (`bs="fs"`) basis setup works in both `gamm()` and `gamm4()`. However, as the **nlme** package does not support crossed random effects, it is not possible to have two factor-smooth interaction terms for the same grouping variable in `gamm()` models (e.g., `y~s(x1, grp, bs="fs")+s(x2, grp, bs="fs")`. These type of crossed random effects are allowed in **gamm4**. The use of `te()` and `ti()` terms are not possible in **gamm4**, due to
issues with how random effects are specified in the **lme4** package, making it impossible to code models where multiple penalties apply to a single basis function. Instead, for multidimensional group-level smooths, the alternate function `t2()` needs to be used to generate these terms, as it creates tensor products with only a single penalty for each basis function (see `?mgcv::t2` for details on these smoothers, and @wood_straightforward_2012 for the theoretical basis behind this type of tensor product). For instance, model 2 for the
bird movement data we discussed in section III would need to be coded as:

```
bird_mod4_gamm4 <-
  gamm4(count ~ t2(week, latitude, species, bs=c("cc", "tp", "re"),
                   k=c(10, 10, 6), m=2),
        data=bird_move, family="poisson")
```

These packages also do not support the same range of families for the dependent
variable; `gamm()` only supports non-Gaussian families by using a fitting method
called penalized quasi-likelihood (PQL) that is slower and not as numerically
stable as the methods used in `gam()`, `bam()`, and `gamm4()`. Non-Gaussian families are well supported by **lme4** (and thus **gamm4**), but can only fit them using marginal likelihood (ML) rather than REML, so may tend to over-smooth relative to `gam()` using REML estimation. Further, neither `gamm()` nor `gamm4()` supports several of the extended families available through **mgcv**, such as zero-inflated, negative binomial, or ordered categorical and multinomial distributions.

## Estimation issues when fitting both global and groupwise smooths

When fitting models with separate global and groupwise smooths (models 2 and 3),
one issue to be aware of is concurvity between the global smooth and groupwise
terms. Concurvity measures how well one smooth term can be approximated by some
combination of the other smooth terms in the model (see `?mgcv::concurvity` for
details). For models 2 and 3, the global term is entirely concurved with the
groupwise smooths. This is because, in the absence of the global smooth term, it
would be possible to recreate that average effect by shifting all the groupwise
smooths so they were centered around the global mean. In practical terms, this
has the consequence of increasing uncertainty around the global mean relative to
a model with only a global smooth. In some cases, it can result in the estimated
global smooth being close to flat, even in simulated examples with a known
strong global effect. This concurvity issue may also increase the time it takes
to fit these models (for example, compare the time it takes to fit models 3 and
5 in Table \ref{tab:comp_time}). These models can still be estimated
because of penalty terms;  all of the methods we have discussed for fitting
model 2 (factor smooth terms or random effect tensor products) automatically create a penalty for the null space of the group-level terms, so that only the global term has its own unpenalized null space. Both the REML and ML criteria work
to balance penalties between nested smooth terms (this is why nested random
effects can be fitted). We have observed that **mgcv** still occasionally
finds solutions with simulated data where the global term is over-smoothed.

To avoid this issue, we recommend both careful choice of basis and setting model degrees of freedom so that groupwise terms are either slightly less flexible than the global term or have a smaller null space. In the examples in section III, we used smoothers with an unpenalized null space (standard thin plate splines) for the global smooth and ones with no null space for the groupwise terms[^gsnull]. When using thin plate splines, it may also help to use splines with a lower order of derivative penalized in the groupwise smooths than the global smooths, as lower-order "tp" splines have fewer basis functions in the null space. For example, we used `m=2` (penalizing squared second derivatives) for the global smooth, and `m=1` (penalizing squared first derivatives) for groupwise smooths in models 2 and 3. Another option is to use a lower number of basis functions (`k`) for groupwise relative to global terms.  This will reduce the maximum flexibility possible in the groupwise terms. We do caution that these are just rules of thumb. In cases where an accurately estimated global smooth is essential, we recommend either fitting model 1 or using specialized functional regression software such as the *refund* package [@scheipl_functional_2014], which enforces constraints on the groupwise smooths so that they always sum to zero at any given point (avoiding the collinearity issue). Also, see below for more information on functional regression.

[^gsnull]: For model 2 both the factor smooth, and tensor products of random effect ("re") and other smooth terms do not have a penalized nullspace by construction (they are full rank), as noted above. For model 3 groupwise terms, we used basis types that had a penalty added to the nullspace, so called "shrinkage" methods: `bs="tp"`, `"cs"`, or `"ps"` have this property.

## A brief foray into the land of Bayes

As mentioned in section II, the penalty matrix can also be treated as the inverse of a prior covariance matrix for model parameters $\boldsymbol{\beta}$.
Intuitively, the basis functions and penalty we use form a prior (in the informal sense) on how we'd like our model term to behave. REML gives an empirical Bayes estimate of the smooth model [@laird_random-effects_1982], where terms in the
null space of the smooth have improper, flat priors (i.e., any value for these terms are considered equally likely), any terms in the range space are treated as having a multivariate normal distribution, and the penalty terms are treated as having an improper flat prior (see
-@wood_generalized_2017 Section 5.8 for more details on this connection). The posterior Bayesian covariance matrix for model parameters can be extracted from any fitted `gam()` or `bam()` model with `vcov(model)`. This can in turn be used
to generate samples from the posterior distribution of the model, as the
Bayesian covariance matrix already incorporates the uncertainty from having to
estimate the covariance matrix into it [the standard confidence intervals used in
**mgcv** are in fact Bayesian posterior credible intervals, which happen to have good frequentist properties; @wood_confidence_2006; @marra_coverage_2012]. Viewing our GAM as Bayesian is a somewhat unavoidable consequence of the equivalence of random effects and splines --- if we think that there is some true smooth that we wish to estimate, we must take a Bayesian view of our random effects (splines) as we do not think that the true smooth changes each time we collect data [@wood_generalized_2017, Section 5.8].

This also means that HGAMs can also be included as components in a more complex
fully Bayesian model. The **mgcv** package includes a function `jagam()` that can
take a specified model formula and automatically convert it into code for the JAGS
(or BUGS) Bayesian statistical packages, which can be adapted by the user to
their own needs.

Similarly, the **brms** package [@burkner_brms:_2017], which can fit complex statistical models using the Bayesian software **Stan** [@carpenter_stan:_2017] allows for the inclusion of **mgcv**-style smooth terms as part of the model specification. The **brms** package does not currently support `te()` tensor products or factor-smooth interaction terms, but does support `t2()`-style tensor products, which means all of the models fitted in this paper can be fit by **brms**.

## Beyond HGAMs: functional regression

The HGAMs we have discussed are actually a type of *functional regression*, which is an extension of standard regression models to cases where the outcome variable $y_i$ and/or the predictor variables $x_i$ for a given outcome are functions, rather than single variables [@ramsay_functional_2005]. HGAMs as we have described them are a form of function-on-scalar regression [@ramsay_functional_2005; @reiss_fast_2010], where we are trying to estimate a smooth function that varies between grouping levels.

We have deliberately focused our paper on these simpler classes of functional regression model, and chosen to use the term HGAM rather than functional regression, as we believe that this more clearly connects these models to modelling approaches already familiar to ecologists. Further, we consider the unit of analysis to still be individual observations, as compared to functional regression where the the unit of analysis is whole functions. For instance, we are interested in applications such as species distribution modelling, where the presence of a given species may be predicted from a sum of several species-specific functions of different environmental variables. However, there is an extensive literature dedicated to the estimation of more complex functional regression models for any interested reader (see @ramsay_functional_2005 for a good introduction, and @scheipl_generalized_2016 for more recent work in this field). The `refund` package [@reiss_fast_2010; @scheipl_functional_2014; @scheipl_generalized_2016] uses the statistical machinery of **mgcv** to fit these models, and should be usable by anyone familiar with **mgcv** modelling syntax.
Functional regression is also a major area of study in Bayesian statistics (e.g., @kaufman_bayesian_2010).



